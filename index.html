<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Zhaomin Wu </title> <meta name="author" content="Zhaomin Wu"> <meta name="description" content="Zhaomin Wu is a Research Fellow in Institute of Data Science, National University of Singapore. He obtained his Ph.D. at the department of computer science in National University of Singapore (NUS), supervised by Prof. Bingsheng He. He obtained his Bachelor degree at Huazhong University of Science at Technology (HUST). His research interest lies in privacy-preserving machine learning, including federated learning, differential privacy, and machine unlearning. He received the Dean's Graduate Research Excellence Award in 2023. "> <meta name="keywords" content="federated learning, machine unlearning, differential privacy"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?4e78a919a56fd48a9089bbae370f534d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.zhaominwu.com/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="https://dblp.org/pid/254/0918" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> <a href="mailto:%7A%68%61%6F%6D%69%6E@%6E%75%73.%65%64%75.%73%67" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/JerryLife" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/zhaomin-wu-958159258" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://orcid.org/0000-0002-6463-0031" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> <a href="https://scholar.google.com/citations?user=QjehmgkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">Service </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Zhaomin</span> Wu </h1> <p class="desc">Research Fellow. <a href="https://www.nus.edu.sg/" rel="external nofollow noopener" target="_blank">National University of Singapore</a>. zhaomin@nus.edu.sg</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/avatar-480.webp 480w,/assets/img/avatar-800.webp 800w,/assets/img/avatar-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/avatar.jpg?aef53b0ef58951102436125d0b56fdbb" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="avatar.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>Dr. Zhaomin Wu (吴肇敏) is a Research Fellow at the Department of Computer Science, National University of Singapore (NUS). He completed his Ph.D. in Computer Science at the National University of Singapore (NUS) in 2024 advised by Prof. <a href="https://www.comp.nus.edu.sg/~hebs/" rel="external nofollow noopener" target="_blank">Bingsheng He</a>. He received his Bachelor’s degree from Huazhong University of Science and Technology (HUST) in 2019.</p> <p>Dr. Wu’s research focuses on trustworthy machine learning, with specific interests in trustworthy AI, federated learning, and machine unlearning. His work has been recognized with awards such as the <a href="https://sigmod.org/sigmod-awards/sigmod-best-artifact-award/" rel="external nofollow noopener" target="_blank">SIGMOD Honorable Mention for Best Artifact</a> (2023), the <a href="https://www.comp.nus.edu.sg/programmes/pg/awards/deans-research/" rel="external nofollow noopener" target="_blank">Dean’s Graduate Research Excellence Award</a> (NUS, 2023), an <a href="https://www.comp.nus.edu.sg/programmes/pg/awards/deans-research/" rel="external nofollow noopener" target="_blank">Honorable Mention for Best Ph.D. Thesis Award</a> (NUS, 2024), and the <a href="https://ids.nus.edu.sg/gathering25.html" rel="external nofollow noopener" target="_blank">Best Research Staff Award</a> (NUS, 2025). His publications appear in top-tier venues including NeurIPS, ICLR, SIGMOD, KDD, WWW, ACL, EMNLP, AAAI, MLSys, and TKDE.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vh; overflow-y: auto;"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jan 26, 2026</th> <td> I was invited to give a <strong>keynote</strong> entitled “<a href="https://sites.google.com/view/flca/keynotes" rel="external nofollow noopener" target="_blank">Bridging Data Silos with Practical Federated Learning</a>” at the <strong>AAAI 2026 FLCA Workshop</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 26, 2026</th> <td> Two papers accepted to <strong>ICLR 2026</strong>. <ul> <li>Zhaomin Wu, Haodong Zhao, Ziyang Wang, Jizhou Guo, Qian Wang, Bingsheng He. <a href="https://openreview.net/forum?id=UIxHaAqFqQ" rel="external nofollow noopener" target="_blank">LLM DNA: Tracing Model Evolution via Functional Representations</a> </li> <li>Zhaomin Wu, Mingzhe Du, Ng See-Kiong, Bingsheng He. <a href="https://openreview.net/forum?id=PDBBYwd1LY" rel="external nofollow noopener" target="_blank">Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts</a> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 13, 2026</th> <td> One paper accepted in <strong>WWW 2026</strong>. <ul> <li>Yicheng Zhang, Zhen Qin, Zhaomin Wu, Jian Hou, Shuiguang Deng. <a href="https://arxiv.org/pdf/2411.19128" rel="external nofollow noopener" target="_blank">Personalized Federated Fine-Tuning for LLMs via Data-Driven Heterogeneous Model Architectures.</a> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 24, 2025</th> <td> One paper accepted in <strong>KDD 2026</strong>. <ul> <li>Jizhou Guo, Zhaomin Wu, Hanchen Yang, Philip S. Yu. <a href="https://arxiv.org/pdf/2505.12225" rel="external nofollow noopener" target="_blank">Mining Intrinsic Rewards from LLM Hidden States for Efficient Best-of-N Sampling</a> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 21, 2025</th> <td> One paper accepted in <strong>EMNLP 2025</strong>. <ul> <li>Zhaomin Wu*, Jizhou Guo*, Junyi Hou, Bingsheng He, Lixin Fan, Qiang Yang. <a href="https://arxiv.org/pdf/2410.10481" rel="external nofollow noopener" target="_blank">Model-based Large Language Model Customization as Service</a> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">May 29, 2025</th> <td> I delivered an <strong>invited talk</strong> entitled “<a href="https://dasfaa2025.github.io/#/program/trust-day" rel="external nofollow noopener" target="_blank">Towards Practical Vertical Federated Learning Systems</a>” on behalf of Prof. Bingsheng He at the <strong>DASFAA 2025</strong> Trust Day. </td> </tr> <tr> <th scope="row" style="width: 20%">May 17, 2025</th> <td> One paper accepted in <strong>ACL 2025</strong>. <ul> <li>Zhen Qin, Zhaomin Wu, Bingsheng He, Shuiguang Deng. <a href="https://aclanthology.org/2025.findings-acl.803/" rel="external nofollow noopener" target="_blank">Federated Data-Efficient Instruction Tuning for Large Language Models.</a> </li> </ul> </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications selected-papers"> <ol class="bibliography"> <li> <div class="row first-author venue-iclr"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-bar-iclr">ICLR 2026</abbr> </div> <div id="wu2025llmdna" class="col-sm-8"> <div class="title">LLM DNA: Tracing Model Evolution via Functional Representations</div> <div class="author"> <em>Zhaomin Wu</em>, Haodong Zhao, Ziyang Wang, Jizhou Guo, Qian Wang, and Bingsheng He </div> <div class="periodical"> <em>In The Fourteenth International Conference on Learning Representations (ICLR)</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2509.24496" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu2025llmdna</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LLM DNA: Tracing Model Evolution via Functional Representations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Zhaomin and Zhao, Haodong and Wang, Ziyang and Guo, Jizhou and Wang, Qian and He, Bingsheng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Fourteenth International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=UIxHaAqFqQ}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row first-author venue-iclr"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-bar-iclr">ICLR 2026</abbr> </div> <div id="wu2025deception" class="col-sm-8"> <div class="title">Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts</div> <div class="author"> <em>Zhaomin Wu</em>, Mingzhe Du, Ng See-Kiong, and Bingsheng He </div> <div class="periodical"> <em>In The Fourteenth International Conference on Learning Representations (ICLR)</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.arxiv.org/pdf/2508.06361" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu2025deception</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Zhaomin and Du, Mingzhe and See-Kiong, Ng and He, Bingsheng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Fourteenth International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=PDBBYwd1LY}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row first-author venue-emnlp"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-bar-emnlp">EMNLP 2025</abbr> </div> <div id="wu2025llamdex" class="col-sm-8"> <div class="title">Model-based Large Language Model Customization as Service</div> <div class="author"> <em>Zhaomin Wu<sup>*</sup></em>, Jizhou Guo<sup>*</sup>, Junyi Hou, Bingsheng He, Lixin Fan, and Qiang Yang </div> <div class="periodical"> <em>In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2410.10481" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Prominent Large Language Model (LLM) services from providers like OpenAI and Google excel at general tasks but often underperform on domain-specific applications. Current customization services for these LLMs typically require users to upload data for fine-tuning, posing significant privacy risks. While differentially private (DP) data synthesis presents a potential alternative, its application commonly results in low effectiveness due to the introduction of excessive noise on data for DP. To overcome this, we introduce Llamdex, a novel framework that facilitates LLM customization as a service, where the client uploads pre-trained domain-specific models rather than data. This client-uploaded model, optionally protected by DP with much lower noise, is inserted into the base LLM via connection modules. Significantly, these connecting modules are trained without requiring sensitive domain data, enabling clients to customize LLM services while preserving data privacy. Experiments demonstrate that Llamdex improves domain-specific accuracy by up to 26% over state-of-the-art private data synthesis methods under identical privacy constraints and, by obviating the need for users to provide domain context within queries, maintains inference efficiency comparable to the original LLM service.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu2025llamdex</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Zhaomin and Guo, Jizhou and Hou, Junyi and He, Bingsheng and Fan, Lixin and Yang, Qiang}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Model-based Large Language Model Customization as Service}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row first-author venue-neurips"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-bar-neurips">NeurIPS 2024</abbr> </div> <div id="wu2024fet" class="col-sm-8"> <div class="title">Federated Transformer: Multi-Party Vertical Federated Learning on Practical Fuzzily Linked Data</div> <div class="author"> <em>Zhaomin Wu</em>, Junyi Hou, Yiqun Diao, and Bingsheng He </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2410.17986" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Xtra-Computing/FeT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Federated Learning (FL) is an evolving paradigm that enables multiple parties to collaboratively train models without sharing raw data. Vertical Federated Learning (VFL), which involves multiple parties contributing distinct features of a shared instance group, is prevalent in real-world, cross-organizational collaborations. In such setups, parties are typically linked by fuzzy identifiers, a common scenario in practice termed as \textitmulti-party fuzzy VFL. Existing models generally address either multi-party VFL or fuzzy VFL between two parties. Extending these models to the practical multi-party fuzzy VFL typically results in significant performance degradation and increased costs for maintaining privacy. To overcome these limitations, we introduce the \textitFederated Transformer (FeT), a novel framework designed to support multi-party VFL with fuzzy identifiers. FeT encodes identifiers into data representations and conducts training using a transformer architecture distributed across different parties, incorporating three new techniques to enhance performance. Additionally, we have developed a scalable privacy framework that integrates differential privacy with secure multi-party computation, effectively protecting local representations at manageable costs. Experiments show that the FeT surpasses the performance of baseline models by up to 46 percentage points when scaled to 50 parties. Additionally, FeT outperforms cutting-edge models in two-party fuzzy VFL settings, while offering improved privacy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu2024fet</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Zhaomin and Hou, Junyi and Diao, Yiqun and He, Bingsheng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Federated Transformer: Multi-Party Vertical Federated Learning on Practical Fuzzily Linked Data}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{36}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row first-author venue-iclr"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-bar-iclr">ICLR 2024</abbr> </div> <div id="wu2024vertibench" class="col-sm-8"> <div class="title">VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks</div> <div class="author"> <em>Zhaomin Wu</em>, Junyi Hou, and Bingsheng He </div> <div class="periodical"> <em>In The Twelfth International Conference on Learning Representations</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=glwwbaeKm2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Xtra-Computing/VertiBench" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://vertibench.xtra.science/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Vertical Federated Learning (VFL) is a crucial paradigm for training machine learning models on feature-partitioned, distributed data. However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions. Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment. This paper addresses these shortcomings by introducing two key factors affecting VFL performance - feature importance and feature correlation - and proposing associated evaluation metrics and dataset splitting methods. Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios. Our comprehensive evaluation of cutting-edge VFL algorithms provides valuable insights for future research in the field.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu2024vertibench</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Zhaomin and Hou, Junyi and He, Bingsheng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=glwwbaeKm2}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row first-author venue-sigmod"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-bar-sigmod">SIGMOD 2023</abbr> <div class="award-trophy" title="Honorable Mention for Best Artifact"> <i class="fa-solid fa-trophy"></i> </div> </div> <div id="10.1145/3589313" class="col-sm-8"> <div class="title">DeltaBoost: Gradient Boosting Decision Trees with Efficient Machine Unlearning</div> <div class="author"> <em>Zhaomin Wu</em>, Junhui Zhu, Qinbin Li, and Bingsheng He </div> <div class="periodical"> <em>Proc. ACM Manag. Data</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Honorable Mention for Best Artifact</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3589313" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3589313" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Xtra-Computing/DeltaBoost/blob/main/DeltaBoost_Technical_Report.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://github.com/Xtra-Computing/DeltaBoost" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="award hidden d-print-inline"> <p></p> <p>DeltaBoost has won Honorable Mention for Best Artifact Award in SIGMOD 2023</p> </div> <div class="abstract hidden"> <p>As machine learning (ML) has been widely developed in real-world applications, the privacy of ML models draws an increasing concern. In this paper, we study how to forget specific data records from ML models to preserve the privacy of these data. Although some studies propose efficient unlearning algorithms on random forests and extremely randomized trees, Gradient Boosting Decision Trees (GBDT), which are widely used in practice, have not been explored. The efficient unlearning of GBDT faces two major challenges: 1) the training of each tree is deterministic and non-robust; 2) the training of a tree depends on all the previous trees. To solve the first challenge, we propose a robust GBDT-like ML model DeltaBoost that enables efficient and accurate deletion according to our theoretical analysis. For the second challenge, we design a training algorithm for DeltaBoost that minimizes the dependency among trees. Our experiments on five datasets demonstrate that DeltaBoost can remove data records from the trained model efficiently and effectively. Our unlearning approach achieves up to two orders of magnitude speedup compared to retraining GBDT. Besides, DeltaBoost produces competitive performance to existing decision-tree-based ML models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1145/3589313</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Zhaomin and Zhu, Junhui and Li, Qinbin and He, Bingsheng}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DeltaBoost: Gradient Boosting Decision Trees with Efficient Machine Unlearning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">issue_date</span> <span class="p">=</span> <span class="s">{June 2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3589313}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3589313}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proc. ACM Manag. Data}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{168}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{26}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{data deletion, gradient boosting decision trees, machine unlearning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row first-author venue-neurips"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100 venue-bar-neurips">NeurIPS 2022</abbr> </div> <div id="NEURIPS2022_84b74416" class="col-sm-8"> <div class="title">A Coupled Design of Exploiting Record Similarity for Practical Vertical Federated Learning</div> <div class="author"> <em>Zhaomin Wu</em>, Qinbin Li, and Bingsheng He </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/84b744165a0597360caad96b06e69313-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://arxiv.org/pdf/2106.06312.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://github.com/Xtra-Computing/FedSim" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Federated learning is a learning paradigm to enable collaborative learning across different parties without revealing raw data. Notably, vertical federated learning (VFL), where parties share the same set of samples but only hold partial features, has a wide range of real-world applications. However, most existing studies in VFL disregard the" record linkage”process. They design algorithms either assuming the data from different parties can be exactly linked or simply linking each record with its most similar neighboring record. These approaches may fail to capture the key features from other less similar records. Moreover, such improper linkage cannot be corrected by training since existing approaches provide no feedback on linkage during training. In this paper, we design a novel coupled training paradigm, FedSim, that integrates one-to-many linkage into the training process. Besides enabling VFL in many real-world applications with fuzzy identifiers, FedSim also achieves better performance in traditional VFL tasks. Moreover, we theoretically analyze the additional privacy risk incurred by sharing similarities. Our experiments on eight datasets with various similarity metrics show that FedSim outperforms other state-of-the-art baselines. The codes of FedSim are available at https://github. com/Xtra-Computing/FedSim.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NEURIPS2022_84b74416</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Zhaomin and Li, Qinbin and He, Bingsheng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{21087--21100}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Coupled Design of Exploiting Record Similarity for Practical Vertical Federated Learning}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.neurips.cc/paper_files/paper/2022/file/84b744165a0597360caad96b06e69313-Paper-Conference.pdf}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Zhaomin Wu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-QXDFT2399G"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-QXDFT2399G');
  </script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>